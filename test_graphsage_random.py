"""
GraphSAGE + Matrix Factorization éšæœºæµ‹è¯•è„šæœ¬
æ¯æ¬¡è®­ç»ƒéšæœºæ‰“ä¹±112ä¸ªæ¨¡å‹ï¼Œå‰90ä¸ªä½œä¸ºè®­ç»ƒï¼Œå22ä¸ªä½œä¸ºæµ‹è¯•
æ¯ä¸ªepochåè¿›è¡Œæµ‹è¯•å¹¶ä¿å­˜ç»“æœ
"""
import argparse
import torch
import pandas as pd
import numpy as np
import random
from models.Embedllm_GraphSAGE import GraphSAGE_TextMF_dyn, build_model_graph
from utils.load_and_process_data import build_full_response_matrix, get_unseen_model_responses
from utils.evaluate import evaluate_graphsage
from torch.optim import Adam
from torch import nn
from datetime import datetime
import os
import sys
from tqdm import tqdm

class TeeOutput:
    """åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶çš„ç±»"""
    def __init__(self, file_path):
        self.file = open(file_path, 'w', encoding='utf-8')
        self.stdout = sys.stdout
        
    def write(self, text):
        self.stdout.write(text)
        self.file.write(text)
        self.file.flush()
        
    def flush(self):
        self.stdout.flush()
        self.file.flush()
        
    def close(self):
        self.file.close()

def set_random_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒå¯é‡å¤"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

def build_train_response_matrix_with_progress(train_data, shuffled_model_ids, save_path=None):
    """
    åªä½¿ç”¨è®­ç»ƒæ•°æ®æ„å»ºå“åº”çŸ©é˜µï¼Œé¿å…æ•°æ®æ³„éœ²ï¼Œå¹¶æ˜¾ç¤ºè¿›åº¦æ¡
    """
    print("ğŸ”— æ„å»ºè®­ç»ƒå“åº”çŸ©é˜µï¼ˆä»…ä½¿ç”¨è®­ç»ƒæ•°æ®ï¼‰...")
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ä¿å­˜çš„å“åº”çŸ©é˜µ
    if save_path and os.path.exists(save_path):
        print(f"   - å‘ç°å·²ä¿å­˜çš„å“åº”çŸ©é˜µ: {save_path}")
        try:
            saved_data = torch.load(save_path, weights_only=True)
            response_matrix = saved_data['response_matrix']
            saved_model_ids = saved_data['model_ids']
            all_prompt_ids = saved_data['prompt_ids']
            
            # æ£€æŸ¥æ¨¡å‹IDæ˜¯å¦åŒ¹é…
            if np.array_equal(saved_model_ids, shuffled_model_ids):
                print(f"   - æ¨¡å‹IDåŒ¹é…ï¼Œç›´æ¥åŠ è½½å“åº”çŸ©é˜µ: {response_matrix.shape}")
                return response_matrix, all_prompt_ids
            else:
                print(f"   - æ¨¡å‹IDä¸åŒ¹é…ï¼Œé‡æ–°æ„å»ºå“åº”çŸ©é˜µ")
        except Exception as e:
            print(f"   - åŠ è½½å¤±è´¥: {e}ï¼Œé‡æ–°æ„å»ºå“åº”çŸ©é˜µ")
    
    # è·å–æ‰€æœ‰prompt ID
    all_prompt_ids = sorted(train_data["prompt_id"].unique())
    num_models = len(shuffled_model_ids)
    num_prompts = len(all_prompt_ids)
    
    # åˆ›å»ºIDæ˜ å°„
    model_id_to_idx = {model_id: idx for idx, model_id in enumerate(shuffled_model_ids)}
    prompt_id_to_idx = {prompt_id: idx for idx, prompt_id in enumerate(all_prompt_ids)}
    
    # åˆå§‹åŒ–å“åº”çŸ©é˜µ
    response_matrix = torch.zeros(num_models, num_prompts, dtype=torch.float32)
    
    print(f"   - å“åº”çŸ©é˜µå½¢çŠ¶: {response_matrix.shape}")
    print(f"   - å¤„ç†è®­ç»ƒæ•°æ®...")
    
    # ä½¿ç”¨è¿›åº¦æ¡å¤„ç†æ•°æ®
    for _, row in tqdm(train_data.iterrows(), total=len(train_data), desc="æ„å»ºå“åº”çŸ©é˜µ"):
        model_id = row["model_id"]
        prompt_id = row["prompt_id"]
        label = row["label"]
        
        if model_id in model_id_to_idx and prompt_id in prompt_id_to_idx:
            model_idx = model_id_to_idx[model_id]
            prompt_idx = prompt_id_to_idx[prompt_id]
            response_matrix[model_idx, prompt_idx] = label
    
    # ä¿å­˜å“åº”çŸ©é˜µ
    if save_path:
        print(f"   - ä¿å­˜å“åº”çŸ©é˜µåˆ°: {save_path}")
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        torch.save({
            'response_matrix': response_matrix,
            'model_ids': shuffled_model_ids,
            'prompt_ids': all_prompt_ids
        }, save_path)
    
    return response_matrix, all_prompt_ids

def create_random_data_loaders(train_data, test_data, batch_size=64, random_seed=None):
    """
    åˆ›å»ºéšæœºæ‰“ä¹±çš„æ•°æ®åŠ è½½å™¨
    æ¯æ¬¡è°ƒç”¨éƒ½ä¼šé‡æ–°éšæœºæ‰“ä¹±æ¨¡å‹é¡ºåº
    """
    from torch.utils.data import Dataset, DataLoader
    
    if random_seed is not None:
        torch.manual_seed(random_seed)
        np.random.seed(random_seed)
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®ä»¥è·å–å®Œæ•´çš„æ¨¡å‹åˆ—è¡¨
    all_data = pd.concat([train_data, test_data], ignore_index=True)
    all_model_ids = sorted(all_data["model_id"].unique())
    
    # éšæœºæ‰“ä¹±æ¨¡å‹ID
    shuffled_model_ids = np.random.permutation(all_model_ids)
    
    # åˆ†å‰²ï¼šå‰90ä¸ªä½œä¸ºè®­ç»ƒæ¨¡å‹ï¼Œå22ä¸ªä½œä¸ºæµ‹è¯•æ¨¡å‹
    train_model_ids = shuffled_model_ids[:90]
    test_model_ids = shuffled_model_ids[90:112]
    
    print(f"è®­ç»ƒæ¨¡å‹ID (å‰90ä¸ª): {train_model_ids[:10]}... (æ˜¾ç¤ºå‰10ä¸ª)")
    print(f"æµ‹è¯•æ¨¡å‹ID (å22ä¸ª): {test_model_ids}")
    
    # è¿‡æ»¤æ•°æ®
    train_data_filtered = train_data[train_data["model_id"].isin(train_model_ids)]
    test_data_filtered = test_data[test_data["model_id"].isin(test_model_ids)]
    
    # åˆ›å»ºIDæ˜ å°„ (å°†åŸå§‹IDæ˜ å°„åˆ°0-111çš„è¿ç»­ID)
    id_mapping = {original_id: new_id for new_id, original_id in enumerate(shuffled_model_ids)}
    
    num_prompts = int(max(max(train_data["prompt_id"]), max(test_data["prompt_id"]))) + 1
    
    class CustomDataset(Dataset):
        def __init__(self, data, id_mapping):
            # å°†åŸå§‹æ¨¡å‹IDæ˜ å°„åˆ°æ–°çš„è¿ç»­ID
            mapped_model_ids = [id_mapping[mid] for mid in data["model_id"].values]
            self.models = torch.tensor(mapped_model_ids, dtype=torch.int64)
            self.prompts = torch.tensor(data["prompt_id"].to_numpy(), dtype=torch.int64)
            self.labels = torch.tensor(data["label"].to_numpy(), dtype=torch.int64)
            
            self.num_models = len(data["model_id"].unique())
            self.num_prompts = num_prompts
            self.num_classes = len(data["label"].unique())

        def __len__(self):
            return len(self.models)

        def __getitem__(self, index):
            return self.models[index], self.prompts[index], self.labels[index]

    train_dataset = CustomDataset(train_data_filtered, id_mapping)
    test_dataset = CustomDataset(test_data_filtered, id_mapping)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader, shuffled_model_ids, id_mapping

def build_unseen_response_matrix_from_train(train_data, shuffled_model_ids, all_prompt_ids):
    """
    ä»è®­ç»ƒæ•°æ®ä¸­ä¸ºæœªè§æ¨¡å‹ï¼ˆå22ä¸ªï¼‰æ„å»ºå“åº”çŸ©é˜µï¼Œé¿å…æ•°æ®æ³„éœ²
    """
    print("ğŸ”— ä»è®­ç»ƒæ•°æ®æ„å»ºæœªè§æ¨¡å‹å“åº”çŸ©é˜µï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰...")
    
    # è·å–æœªè§æ¨¡å‹IDï¼ˆæ‰“ä¹±åçš„å22ä¸ªï¼‰
    unseen_model_ids = shuffled_model_ids[90:112]
    
    # åˆ›å»ºprompt IDæ˜ å°„
    prompt_id_to_idx = {prompt_id: idx for idx, prompt_id in enumerate(all_prompt_ids)}
    
    # åˆå§‹åŒ–æœªè§æ¨¡å‹å“åº”çŸ©é˜µï¼ˆåªåŒ…å«å22ä¸ªæ¨¡å‹ï¼‰
    unseen_response_matrix = torch.zeros(22, len(all_prompt_ids), dtype=torch.float32)
    
    # ä»è®­ç»ƒæ•°æ®ä¸­è¿‡æ»¤æœªè§æ¨¡å‹çš„æ•°æ®
    train_data_filtered = train_data[train_data["model_id"].isin(unseen_model_ids)]
    
    print(f"   - æœªè§æ¨¡å‹å“åº”çŸ©é˜µå½¢çŠ¶: {unseen_response_matrix.shape}")
    print(f"   - æœªè§æ¨¡å‹ID: {unseen_model_ids}")
    print(f"   - ä»è®­ç»ƒæ•°æ®ä¸­æå–æœªè§æ¨¡å‹å“åº”...")
    
    # ä½¿ç”¨è¿›åº¦æ¡å¤„ç†æ•°æ®
    for _, row in tqdm(train_data_filtered.iterrows(), total=len(train_data_filtered), desc="æ„å»ºæœªè§æ¨¡å‹å“åº”çŸ©é˜µ"):
        model_id = row["model_id"]
        prompt_id = row["prompt_id"]
        label = row["label"]
        
        if model_id in unseen_model_ids and prompt_id in prompt_id_to_idx:
            # å°†æ¨¡å‹IDæ˜ å°„åˆ°æœªè§æ¨¡å‹çŸ©é˜µçš„ç´¢å¼•ï¼ˆ0-21ï¼‰
            unseen_model_idx = list(unseen_model_ids).index(model_id)
            prompt_idx = prompt_id_to_idx[prompt_id]
            unseen_response_matrix[unseen_model_idx, prompt_idx] = label
    
    print(f"   - æˆåŠŸæ„å»ºæœªè§æ¨¡å‹å“åº”çŸ©é˜µï¼Œæ•°æ®æ¥æºï¼šè®­ç»ƒé›†")
    return unseen_response_matrix

def train_graphsage_with_epoch_testing(model, graph_data, train_loader, test_loader, 
                                     num_epochs, lr, device, unseen_responses=None, 
                                     weight_decay=1e-5, log_file=None, contrastive_weight=0.1):
    """
    è®­ç»ƒGraphSAGEæ¨¡å‹ï¼Œæ¯ä¸ªepochåè¿›è¡Œæµ‹è¯•ï¼Œæ”¯æŒå›¾å¯¹æ¯”å­¦ä¹ 
    """
    optimizer = Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    loss_fn = nn.CrossEntropyLoss()
    
    # è®°å½•è®­ç»ƒå†å²
    train_history = []
    test_history = []
    contrastive_loss_history = []
    max_accuracy = 0
    best_epoch = 0
    
    print("å¼€å§‹è®­ç»ƒ...")
    print("=" * 80)
    
    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        total_loss = 0
        total_main_loss = 0
        total_contrastive_loss = 0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}")
        
        for models, prompts, labels in progress_bar:
            models, prompts, labels = models.to(device), prompts.to(device), labels.to(device)
            
            # åªè®­ç»ƒå‰90ä¸ªæ¨¡å‹
            train_mask = models < 90
            if train_mask.sum() == 0:
                continue
            
            optimizer.zero_grad()
            
            # å‰å‘ä¼ æ’­ï¼Œè·å–logitså’Œå›¾å¯¹æ¯”å­¦ä¹ æŸå¤±
            output = model(graph_data, models[train_mask], prompts[train_mask], test_mode=False)
            
            if isinstance(output, tuple):
                # è®­ç»ƒæ¨¡å¼ï¼šè¿”å›(logits, contrastive_loss)
                logits, contrastive_loss = output
            else:
                # å…¼å®¹æ€§ï¼šå¦‚æœåªè¿”å›logits
                logits = output
                contrastive_loss = torch.tensor(0.0, device=device)
            
            # ä¸»ä»»åŠ¡æŸå¤±
            main_loss = loss_fn(logits, labels[train_mask])
            
            # æ€»æŸå¤± = ä¸»ä»»åŠ¡æŸå¤± + å›¾å¯¹æ¯”å­¦ä¹ æŸå¤±
            total_batch_loss = main_loss + contrastive_weight * contrastive_loss
            
            total_batch_loss.backward()
            optimizer.step()

            total_loss += total_batch_loss.item()
            total_main_loss += main_loss.item()
            total_contrastive_loss += contrastive_loss.item()
            num_batches += 1
            
            progress_bar.set_postfix({
                'total': f'{total_batch_loss.item():.4f}',
                'main': f'{main_loss.item():.4f}',
                'contrast': f'{contrastive_loss.item():.4f}'
            })
        
        train_loss = total_loss / num_batches if num_batches > 0 else 0
        avg_main_loss = total_main_loss / num_batches if num_batches > 0 else 0
        avg_contrastive_loss = total_contrastive_loss / num_batches if num_batches > 0 else 0
        
        # æµ‹è¯•é˜¶æ®µ
        model.eval()
        test_loss, test_accuracy = evaluate_graphsage(model, graph_data, test_loader, device, unseen_responses)
        
        # æ›´æ–°æœ€ä½³ç»“æœ
        if test_accuracy > max_accuracy:
            max_accuracy = test_accuracy
            best_epoch = epoch + 1
        
        # è®°å½•å†å²
        train_history.append(train_loss)
        test_history.append(test_accuracy)
        contrastive_loss_history.append(avg_contrastive_loss)
        
        # è¾“å‡ºç»“æœ
        result_line = f"Epoch {epoch+1:3d}/{num_epochs} | Train Loss: {train_loss:.6f} (Main: {avg_main_loss:.6f}, Contrast: {avg_contrastive_loss:.6f}) | Test Loss: {test_loss:.6f} | Test Acc: {test_accuracy:.6f} | Best: {max_accuracy:.6f} (Epoch {best_epoch})"
        print(result_line)
        
        # å¦‚æœæœ‰æ—¥å¿—æ–‡ä»¶ï¼Œä¹Ÿå†™å…¥æ–‡ä»¶
        if log_file:
            log_file.write(result_line + "\n")
            log_file.flush()
    
    print("=" * 80)
    print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³å‡†ç¡®ç‡: {max_accuracy:.6f} (ç¬¬ {best_epoch} è½®)")
    
    return max_accuracy, best_epoch, train_history, test_history, contrastive_loss_history

def create_test_args():
    """åˆ›å»ºæµ‹è¯•å‚æ•°"""
    args = argparse.Namespace()
    
    # æ•°æ®è·¯å¾„
    args.train_data_path = "data/train.csv"
    args.test_data_path = "data/test.csv"
    args.question_embedding_path = "data/question_embeddings.pth"
    args.model_embeddings_path = "data/model_embeddings_static.pth"
    
    # æ¨¡å‹å‚æ•°
    args.embedding_dim = 1024  # åŒ¹é…model_embeddings_static.pthçš„ç»´åº¦
    args.alpha = 0.05
    args.batch_size = 64
    args.num_epochs = 50
    args.learning_rate = 0.0005
    args.weight_decay = 1e-5
    
    # GNNå‚æ•°
    args.hidden_dim = 256
    args.num_layers = 2
    args.k_neighbors = 5
    args.gnn_type = "GAT"  # æˆ– "GAT"
    args.num_heads = 8
    
    # åŠ¨æ€åµŒå…¥å‚æ•°
    args.is_dyn = True
    args.frozen = False
    
    # éšæœºç§å­
    args.random_seed = 42
    
    # è¾“å‡ºè®¾ç½®
    current_date = datetime.now().strftime("%Y-%m-%d")
    current_time = datetime.now().strftime("%H-%M-%S")
    output_dir = f"output/{current_date}"
    os.makedirs(output_dir, exist_ok=True)
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    log_filename = f"GraphSAGE_Random_Test_train90_test22_epochs{args.num_epochs}_lr{args.learning_rate}_seed{args.random_seed}_{current_time}.txt"
    args.log_save_path = os.path.join(output_dir, log_filename)
    
    return args

def run_graphsage_random_test():
    """è¿è¡ŒGraphSAGEéšæœºæµ‹è¯•"""
    print("ğŸš€ GraphSAGE + Matrix Factorization éšæœºæµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºå‚æ•°
    args = create_test_args()
    
    # è®¾ç½®éšæœºç§å­
    set_random_seed(args.random_seed)
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = [
        args.train_data_path,
        args.test_data_path,
        args.question_embedding_path,
        args.model_embeddings_path
    ]
    
    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print("âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶:")
        for f in missing_files:
            print(f"   - {f}")
        return False
    
    try:
        print("ğŸ“Š åŠ è½½æ•°æ®é›†...")
        train_data = pd.read_csv(args.train_data_path)
        test_data = pd.read_csv(args.test_data_path)
        question_embeddings = torch.load(args.question_embedding_path)
        model_embeddings = torch.load(args.model_embeddings_path)
        
        num_prompts = question_embeddings.shape[0]
        num_models = 112  # å›ºå®šä¸º112ä¸ªæ¨¡å‹
        
        print(f"   - æ¨¡å‹æ•°é‡: {num_models}")
        print(f"   - é—®é¢˜æ•°é‡: {num_prompts}")
        print(f"   - è®­ç»ƒæ•°æ®å¤§å°: {len(train_data)}")
        print(f"   - æµ‹è¯•æ•°æ®å¤§å°: {len(test_data)}")
        
        # è®¾å¤‡
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"   - ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆ›å»ºéšæœºæ•°æ®åŠ è½½å™¨
        print("\nğŸ”€ åˆ›å»ºéšæœºæ•°æ®åŠ è½½å™¨...")
        train_loader, test_loader, shuffled_model_ids, id_mapping = create_random_data_loaders(
            train_data, test_data, batch_size=args.batch_size, random_seed=args.random_seed
        )
        
        # æ„å»ºè®­ç»ƒå“åº”çŸ©é˜µï¼ˆä»…ä½¿ç”¨è®­ç»ƒæ•°æ®ï¼Œé¿å…æ•°æ®æ³„éœ²ï¼‰
        current_date = datetime.now().strftime("%Y-%m-%d")
        response_matrix_save_path = f"data/response_matrix_{current_date}_seed{args.random_seed}.pth"
        
        train_response_matrix, all_prompt_ids = build_train_response_matrix_with_progress(
            train_data, shuffled_model_ids, save_path=response_matrix_save_path
        )
        
        # æ„å»ºå›¾æ•°æ®ï¼ˆåªä½¿ç”¨å‰90ä¸ªè®­ç»ƒæ¨¡å‹ï¼‰
        print("\nğŸ“ˆ æ„å»ºæ¨¡å‹å›¾...")
        train_response_matrix_90 = train_response_matrix[:90]  # åªç”¨å‰90ä¸ªæ¨¡å‹
        train_model_embeddings = model_embeddings[shuffled_model_ids[:90]]  # å¯¹åº”çš„æ¨¡å‹åµŒå…¥
        
        graph_data = build_model_graph(
            train_response_matrix_90, 
            train_model_embeddings, 
            k_neighbors=args.k_neighbors, 
            device=device
        )
        print(f"   - å›¾èŠ‚ç‚¹æ•°: {graph_data.x.shape[0]}")
        print(f"   - å›¾è¾¹æ•°: {graph_data.edge_index.shape[1]}")
        
        # ä»è®­ç»ƒæ•°æ®æ„å»ºæœªè§æ¨¡å‹å“åº”çŸ©é˜µï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
        unseen_responses = build_unseen_response_matrix_from_train(
            train_data, shuffled_model_ids, all_prompt_ids
        )
        
        # åˆ›å»ºæ¨¡å‹
        print("\nğŸ§  åˆå§‹åŒ–GraphSAGEæ¨¡å‹...")
        # é‡æ–°æ’åˆ—æ¨¡å‹åµŒå…¥ä»¥åŒ¹é…æ–°çš„é¡ºåº
        reordered_embeddings = model_embeddings[shuffled_model_ids]
        
        model = GraphSAGE_TextMF_dyn(
            question_embeddings=question_embeddings,
            model_embedding_dim=args.embedding_dim,
            alpha=args.alpha,
            num_models=num_models,
            num_prompts=num_prompts,
            model_embeddings=reordered_embeddings,
            is_dyn=args.is_dyn,
            frozen=args.frozen,
            hidden_dim=args.hidden_dim,
            num_layers=args.num_layers,
            num_train_models=90,
            gnn_type=args.gnn_type,
            num_heads=args.num_heads
        )
        model.to(device)
        
        # è®¾ç½®è®­ç»ƒå“åº”çŸ©é˜µï¼ˆç”¨äºæœªè§æ¨¡å‹çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼‰
        model.set_train_responses(train_response_matrix_90)
        
        print(f"   - æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")
        print(f"   - å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
        
        # è®­ç»ƒé…ç½®
        print(f"\nâš™ï¸ è®­ç»ƒé…ç½®:")
        print(f"   - è®­ç»ƒæ¨¡å‹: å‰90ä¸ª (ID: {shuffled_model_ids[:5]}...)")
        print(f"   - æµ‹è¯•æ¨¡å‹: å22ä¸ª (ID: {shuffled_model_ids[90:95]}...)")
        print(f"   - åŠ¨æ€åµŒå…¥: {args.is_dyn}")
        print(f"   - å†»ç»“å‚æ•°: {args.frozen}")
        print(f"   - è®­ç»ƒè½®æ•°: {args.num_epochs}")
        print(f"   - å­¦ä¹ ç‡: {args.learning_rate}")
        print(f"   - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
        print(f"   - éšæœºç§å­: {args.random_seed}")
        
        # å¼€å§‹è®­ç»ƒ
        print(f"\nğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
        print(f"ğŸ“ è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: {args.log_save_path}")
        print("=" * 80)
        
        # è®¾ç½®è¾“å‡ºé‡å®šå‘
        tee_output = TeeOutput(args.log_save_path)
        original_stdout = sys.stdout
        
        try:
            # å†™å…¥é…ç½®ä¿¡æ¯åˆ°æ—¥å¿—
            sys.stdout = tee_output
            print("=" * 80)
            print("GraphSAGE + Matrix Factorization éšæœºæµ‹è¯•æ—¥å¿—")
            print("=" * 80)
            print(f"æµ‹è¯•å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print(f"é¡¹ç›®: GraphSAGE + Matrix Factorization")
            print(f"æµ‹è¯•ç±»å‹: éšæœºæ‰“ä¹±æ¨¡å‹é¡ºåºæµ‹è¯•")
            print(f"è®­ç»ƒæ¨¡å‹: å‰90ä¸ª")
            print(f"æµ‹è¯•æ¨¡å‹: å22ä¸ª")
            print(f"éšæœºç§å­: {args.random_seed}")
            print(f"è®­ç»ƒè½®æ•°: {args.num_epochs}")
            print(f"å­¦ä¹ ç‡: {args.learning_rate}")
            print(f"æ‰¹æ¬¡å¤§å°: {args.batch_size}")
            print(f"åŠ¨æ€åµŒå…¥: {args.is_dyn}")
            print(f"å†»ç»“å‚æ•°: {args.frozen}")
            print(f"GNNç±»å‹: {args.gnn_type}")
            print(f"æ³¨æ„åŠ›å¤´æ•°: {args.num_heads}")
            print(f"è®¾å¤‡: {device}")
            print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")
            print(f"å¯è®­ç»ƒå‚æ•°: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")
            print(f"æ‰“ä¹±åçš„æ¨¡å‹é¡ºåº (å‰10ä¸ª): {shuffled_model_ids[:10]}")
            print(f"è®­ç»ƒæ¨¡å‹ID: {shuffled_model_ids[:90]}")
            print(f"æµ‹è¯•æ¨¡å‹ID: {shuffled_model_ids[90:112]}")
            print("=" * 80)
            
            # å¼€å§‹è®­ç»ƒ
            result = train_graphsage_with_epoch_testing(
                model, graph_data, train_loader, test_loader,
                num_epochs=args.num_epochs, lr=args.learning_rate,
                device=device, unseen_responses=unseen_responses.to(device),
                weight_decay=args.weight_decay, log_file=tee_output.file,
                contrastive_weight=0.1  # å›¾å¯¹æ¯”å­¦ä¹ æƒé‡
            )
            
            # å¤„ç†è¿”å›å€¼ï¼ˆå…¼å®¹æ–°æ—§ç‰ˆæœ¬ï¼‰
            if len(result) == 5:
                max_acc, best_epoch, train_history, test_history, contrastive_loss_history = result
            else:
                max_acc, best_epoch, train_history, test_history = result
                contrastive_loss_history = []
            
            print("=" * 80)
            print("è®­ç»ƒæ€»ç»“:")
            print(f"æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {max_acc:.6f}")
            print(f"æœ€ä½³è½®æ¬¡: {best_epoch}")
            print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_history[-1]:.6f}")
            print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_history[-1]:.6f}")
            print(f"æµ‹è¯•å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            print("=" * 80)
            
        finally:
            # æ¢å¤åŸå§‹è¾“å‡º
            sys.stdout = original_stdout
            tee_output.close()
        
        print("=" * 80)
        print(f"âœ… æµ‹è¯•å®Œæˆ!")
        print(f"ğŸ¯ æœ€ä½³å‡†ç¡®ç‡: {max_acc:.6f} (ç¬¬ {best_epoch} è½®)")
        print(f"ğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {args.log_save_path}")
        print("=" * 80)
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = run_graphsage_random_test()
    if success:
        print("\nğŸ‰ GraphSAGEéšæœºæµ‹è¯•å®Œæˆ!")
    else:
        print("\nğŸ’¥ GraphSAGEéšæœºæµ‹è¯•å¤±è´¥!")
