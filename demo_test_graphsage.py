"""
GraphSAGE + Matrix Factorization æ¼”ç¤ºæµ‹è¯•è„šæœ¬
å¿«é€Ÿæµ‹è¯•åŠŸèƒ½æ˜¯å¦æ­£å¸¸ï¼ˆåªè¿è¡Œ5ä¸ªepochï¼‰
"""
import argparse
import torch
import pandas as pd
import numpy as np
import random
from models.Embedllm_GraphSAGE import GraphSAGE_TextMF_dyn, build_model_graph
from utils.load_and_process_data import build_full_response_matrix, get_unseen_model_responses
from utils.evaluate import evaluate_graphsage
from torch.optim import Adam
from torch import nn
from datetime import datetime
import os
import sys
from tqdm import tqdm

def set_random_seed(seed=42):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å®éªŒå¯é‡å¤"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

def create_random_data_loaders(train_data, test_data, batch_size=64, random_seed=None):
    """åˆ›å»ºéšæœºæ‰“ä¹±çš„æ•°æ®åŠ è½½å™¨"""
    from torch.utils.data import Dataset, DataLoader
    
    if random_seed is not None:
        torch.manual_seed(random_seed)
        np.random.seed(random_seed)
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®ä»¥è·å–å®Œæ•´çš„æ¨¡å‹åˆ—è¡¨
    all_data = pd.concat([train_data, test_data], ignore_index=True)
    all_model_ids = sorted(all_data["model_id"].unique())
    
    # éšæœºæ‰“ä¹±æ¨¡å‹ID
    shuffled_model_ids = np.random.permutation(all_model_ids)
    
    # åˆ†å‰²ï¼šå‰90ä¸ªä½œä¸ºè®­ç»ƒæ¨¡å‹ï¼Œå22ä¸ªä½œä¸ºæµ‹è¯•æ¨¡å‹
    train_model_ids = shuffled_model_ids[:90]
    test_model_ids = shuffled_model_ids[90:112]
    
    print(f"è®­ç»ƒæ¨¡å‹ID (å‰5ä¸ª): {train_model_ids[:5]}")
    print(f"æµ‹è¯•æ¨¡å‹ID (å‰5ä¸ª): {test_model_ids[:5]}")
    
    # è¿‡æ»¤æ•°æ®
    train_data_filtered = train_data[train_data["model_id"].isin(train_model_ids)]
    test_data_filtered = test_data[test_data["model_id"].isin(test_model_ids)]
    
    # åˆ›å»ºIDæ˜ å°„
    id_mapping = {original_id: new_id for new_id, original_id in enumerate(shuffled_model_ids)}
    
    num_prompts = int(max(max(train_data["prompt_id"]), max(test_data["prompt_id"]))) + 1
    
    class CustomDataset(Dataset):
        def __init__(self, data, id_mapping):
            mapped_model_ids = [id_mapping[mid] for mid in data["model_id"].values]
            self.models = torch.tensor(mapped_model_ids, dtype=torch.int64)
            self.prompts = torch.tensor(data["prompt_id"].to_numpy(), dtype=torch.int64)
            self.labels = torch.tensor(data["label"].to_numpy(), dtype=torch.int64)

        def __len__(self):
            return len(self.models)

        def __getitem__(self, index):
            return self.models[index], self.prompts[index], self.labels[index]

    train_dataset = CustomDataset(train_data_filtered, id_mapping)
    test_dataset = CustomDataset(test_data_filtered, id_mapping)

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    return train_loader, test_loader, shuffled_model_ids, id_mapping

def demo_test():
    """è¿è¡Œæ¼”ç¤ºæµ‹è¯•"""
    print("ğŸš€ GraphSAGE + Matrix Factorization æ¼”ç¤ºæµ‹è¯•")
    print("=" * 60)
    
    # è®¾ç½®éšæœºç§å­
    set_random_seed(42)
    
    # æ£€æŸ¥æ–‡ä»¶
    required_files = [
        "data/train.csv",
        "data/test.csv",
        "data/question_embeddings.pth",
        "data/model_embeddings_static.pth"
    ]
    
    missing_files = [f for f in required_files if not os.path.exists(f)]
    if missing_files:
        print("âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶:")
        for f in missing_files:
            print(f"   - {f}")
        return False
    
    try:
        print("ğŸ“Š åŠ è½½æ•°æ®...")
        train_data = pd.read_csv("data/train.csv")
        test_data = pd.read_csv("data/test.csv")
        question_embeddings = torch.load("data/question_embeddings.pth", weights_only=True)
        model_embeddings = torch.load("data/model_embeddings_static.pth", weights_only=True)
        
        num_prompts = question_embeddings.shape[0]
        num_models = 112
        
        print(f"   - æ¨¡å‹æ•°é‡: {num_models}")
        print(f"   - é—®é¢˜æ•°é‡: {num_prompts}")
        
        # è®¾å¤‡
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"   - ä½¿ç”¨è®¾å¤‡: {device}")
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("\nğŸ”€ åˆ›å»ºéšæœºæ•°æ®åŠ è½½å™¨...")
        train_loader, test_loader, shuffled_model_ids, id_mapping = create_random_data_loaders(
            train_data, test_data, batch_size=64, random_seed=42
        )
        
        # æ„å»ºå“åº”çŸ©é˜µ
        print("\nğŸ”— æ„å»ºå“åº”çŸ©é˜µ...")
        all_data = pd.concat([train_data, test_data], ignore_index=True)
        all_prompt_ids = sorted(all_data["prompt_id"].unique())
        response_matrix = torch.zeros(num_models, len(all_prompt_ids), dtype=torch.float32)
        
        for _, row in all_data.iterrows():
            if row["model_id"] in shuffled_model_ids:
                model_idx = id_mapping[row["model_id"]]
                prompt_idx = all_prompt_ids.index(row["prompt_id"])
                response_matrix[model_idx, prompt_idx] = row["label"]
        
        print(f"   - å“åº”çŸ©é˜µå½¢çŠ¶: {response_matrix.shape}")
        
        # æ„å»ºå›¾æ•°æ®
        print("\nğŸ“ˆ æ„å»ºæ¨¡å‹å›¾...")
        train_response_matrix = response_matrix[:90]
        train_model_embeddings = model_embeddings[shuffled_model_ids[:90]]
        
        graph_data = build_model_graph(
            train_response_matrix, 
            train_model_embeddings, 
            k_neighbors=10, 
            device=device
        )
        print(f"   - å›¾èŠ‚ç‚¹æ•°: {graph_data.x.shape[0]}")
        print(f"   - å›¾è¾¹æ•°: {graph_data.edge_index.shape[1]}")
        
        # å‡†å¤‡æœªè§æ¨¡å‹å“åº”
        unseen_responses = response_matrix[90:112]
        
        # åˆ›å»ºæ¨¡å‹
        print("\nğŸ§  åˆå§‹åŒ–GraphSAGEæ¨¡å‹...")
        reordered_embeddings = model_embeddings[shuffled_model_ids]
        
        model = GraphSAGE_TextMF_dyn(
            question_embeddings=question_embeddings,
            model_embedding_dim=1024,
            alpha=0.1,
            num_models=num_models,
            num_prompts=num_prompts,
            model_embeddings=reordered_embeddings,
            is_dyn=True,
            frozen=False,
            hidden_dim=256,
            num_layers=2,
            num_train_models=90
        )
        model.to(device)
        model.set_train_responses(train_response_matrix)
        
        print(f"   - æ¨¡å‹å‚æ•°æ€»æ•°: {sum(p.numel() for p in model.parameters()):,}")
        
        # ç®€å•è®­ç»ƒæµ‹è¯•ï¼ˆåªè®­ç»ƒ5ä¸ªepochï¼‰
        print("\nğŸ‹ï¸ å¼€å§‹æ¼”ç¤ºè®­ç»ƒï¼ˆ5ä¸ªepochï¼‰...")
        optimizer = Adam(model.parameters(), lr=0.001)
        loss_fn = nn.CrossEntropyLoss()
        
        for epoch in range(5):
            model.train()
            total_loss = 0
            num_batches = 0
            
            for models, prompts, labels in train_loader:
                models, prompts, labels = models.to(device), prompts.to(device), labels.to(device)
                
                train_mask = models < 90
                if train_mask.sum() == 0:
                    continue
                
                optimizer.zero_grad()
                logits = model(graph_data, models[train_mask], prompts[train_mask], test_mode=False)
                loss = loss_fn(logits, labels[train_mask])
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                num_batches += 1
            
            train_loss = total_loss / num_batches if num_batches > 0 else 0
            
            # æµ‹è¯•
            model.eval()
            test_loss, test_accuracy = evaluate_graphsage(model, graph_data, test_loader, device, unseen_responses.to(device))
            
            print(f"Epoch {epoch+1}/5 | Train Loss: {train_loss:.6f} | Test Loss: {test_loss:.6f} | Test Acc: {test_accuracy:.6f}")
        
        print("\nâœ… æ¼”ç¤ºæµ‹è¯•å®Œæˆï¼")
        print("ğŸ¯ è„šæœ¬åŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥è¿è¡Œå®Œæ•´æµ‹è¯•")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = demo_test()
    if success:
        print("\nğŸ‰ æ¼”ç¤ºæµ‹è¯•æˆåŠŸï¼å¯ä»¥è¿è¡Œå®Œæ•´çš„æµ‹è¯•è„šæœ¬")
        print("è¿è¡Œå®Œæ•´æµ‹è¯•: python test_graphsage_random.py")
    else:
        print("\nğŸ’¥ æ¼”ç¤ºæµ‹è¯•å¤±è´¥ï¼è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
